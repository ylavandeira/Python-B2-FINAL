# -*- coding: utf-8 -*-
"""python_b2_proyecto_final - assigment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14gzsEJ46F64nM5WaRkBGNi53Sdlrc7_I

*Por favor, imagina que estás trabajando en un proyecto como **Científico de Datos** y se te solicita completar la siguiente información.*

## **Información del estudiante**
---
* **Título**: python-b2-20241-proyecto-final-ylavandeira
* **Autor**: Yago Lavandeira Amenedo
* **Correo**: ylavandeira@uoc.edu
* **Fecha**: 21/01/2025
* **Salida**: ipynb, predicciones.csv
---

### **Contexto**:
Este ejercicio se basa en el conjunto de datos GFT Open Finance, cuyo objetivo es fomentar la competencia entre proveedores financieros, impulsar la innovación digital y promover nuevos servicios basados en datos. Este conjunto de datos se centra en la banca abierta y ofrece una valiosa oportunidad de aprendizaje sobre datos y tecnologías abiertas en el sector financiero.

La actividad propuesta permitirá a los estudiantes enfrentar la nueva realidad del intercambio de información bancaria, confrontando bases de datos de diferentes instituciones financieras, incluidas dos bancos y una compañía de seguros.

El objetivo es practicar habilidades de ingeniería de datos y ciencia de datos al desarrollar un sistema que sugiera **la clasificación de los tipos de financiamiento para un cliente específico**. Esto contribuirá a mejorar la experiencia del cliente mediante el diseño de modelos que optimicen la oferta de tipos de financiamiento.

Open Finance representa la evolución de Open Banking y promete traer más transparencia y autonomía a los usuarios del sistema financiero. Los datos proporcionados corresponden a la implementación de Open Finance en Brasil, donde se permite el intercambio de datos sobre seguros, inversiones y fondos de pensiones. El conjunto de datos incluye información financiera de un banco minorista (RetailBankEFG) obtenida a través de financiamiento abierto, con el consentimiento de los clientes, así como datos de un banco de inversión (InvestmentBankCDE) y una compañía de seguros (InsuranceCompanyABC). Estos conjuntos de datos contienen datos de compras anteriores de clientes, así como algunos datos demográficos.

### **Problema:**
El reto consiste en concebir una solución que integre las bases de datos de estas instituciones financieras en el contexto emergente de Open Finance, procese los datos con eficacia y cree modelos de clasificación para tipos de financiamientos utilizando algoritmos de aprendizaje automático.

### **Preguntas:**
Durante el desarrollo del ejercicio encontrarás una o más secciones de preguntas. Por favor, responde de manera justificada y, si se solicita incluir código, investiga para responder adecuadamente.

**Nota:** Para algunas respuestas, debes proporcionar la solución mediante código en Python e implementarla justo debajo de la línea correspondiente.
`#Write your code here`

### **Restricciones:**
No puedes borrar los nombres de las variables propuestas para el desarrollo del ejercicio.

### **Actividades:**
Para elaborar una solución que consolide las bases de datos de estas instituciones financieras y genere modelos de clasificación de tipos de financiamiento mediante algoritmos de aprendizaje automático, podemos dividir el proceso en los siguientes pasos:

- **Preparación de datos:** Integrar y limpiar los datos de las bases de datos de las instituciones financieras, asegurando que estén en un formato adecuado para su análisis.
- **Exploración de datos:** Realizar un análisis exploratorio de los datos para comprender mejor su estructura, distribución y características. Esto puede incluir la visualización de datos y la identificación de posibles patrones o relaciones.
- **Ingesta de datos:** Integrar las bases de datos de las instituciones financieras en un único repositorio de datos, asegurando que la información se pueda acceder de manera eficiente y segura.
- **Procesamiento de datos:** Realizar transformaciones adicionales en los datos según sea necesario, como la codificación de variables categóricas, la normalización de características numéricas y la manipulación de valores faltantes.
- Claro, aquí tienes el texto corregido:
- **Desarrollo de modelos para clasificar los tipos de financiamiento:** Seleccionar y entrenar algoritmos de clasificación adecuados para el problema de clasificación de tipos de financiamiento. Esto puede incluir técnicas como algoritmos de clasificación supervisada, tales como Support Vector Machines, Random Forests o redes neuronales.
- **Validación del modelo:** Evaluar el rendimiento de los modelos utilizando métricas adecuadas, como precisión, recall, F1-score, etc. Utilizar técnicas como la validación cruzada para garantizar la generalización del modelo.
- **Optimización del modelo:** Ajustar hiperparámetros y realizar selección de características para optimizar el rendimiento de los modelos.

## **Solución**:

## **Entorno**:

## **Origen de la fuente de datos**:

Los datos están ubicados en la carpeta "data" y constan de los siguientes archivos.

* InvestmentBankCDE.csv
* RetailBankEFG.csv
* InsuranceCompanyABC.csv

Nota: Los datos proporcionados son ficticios y no se corresponden con la realidad de ninguna manera. A continuación, se muestran algunas de las columnas a utilizar por el modelo.

*Por favor, agrega aquellas columnas que faltan y que se encuentran en el archivo InsuranceCompanyABC.csv*




```python
[
  "seguro auto",
  "seguro vida Emp",
  "seguro vida PF",
  "Seguro Residencial",
  "Investimento Fundos_cambiais",
  "Investimento Fundos_commodities",
  "Investimento LCI",
  "Investimento LCA",
  "Investimento Poupanca",
  "Investimento Fundos Multimercado",
  "Investimento Tesouro Direto",
  "Financiamento Casa",
  "Financiamento Carro",
  "Emprestimo _pessoal",
  "Emprestimo _consignado",
  "Emprestimo _limite_especial",
  "Emprestimo _educacao",
  "Emprestimo _viagem",
  "Investimento CDB",
  "Investimento Fundos"
]
```

# Librerías
Las siguientes son varias de las librerias necesarias para el desarrollo del ejercicio; sin embargo, estas no estan limitadas es decir puedes incluir otras librerias para desarrollar el ejercico.
"""

import re
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.ensemble import (
    RandomForestClassifier,
    VotingClassifier,
    ExtraTreesClassifier,
    AdaBoostClassifier,
    GradientBoostingClassifier
)
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import RidgeClassifier, LogisticRegression
from sklearn.model_selection import (
    train_test_split,
    learning_curve,
    ShuffleSplit,
    cross_val_score,
    KFold
)
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score
)
from functools import reduce
from sklearn.preprocessing import (
    LabelEncoder,
    StandardScaler
)
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import (
    SMOTE,
    RandomOverSampler,
    SMOTENC,
    ADASYN
)
from imblearn.under_sampling import (
    RandomUnderSampler,
    NearMiss
)
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import itertools
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedShuffleSplit

"""## Configuración de visualización de conjuntos de datos"""

pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)

"""### Descarga las fuentes de datos

Si estás utilizando Google Colaboratory o un entorno Linux con la herramienta wget, puedes descomentar las siguientes líneas para descargar los datos.
"""

#!mkdir data
#!wget https://raw.githubusercontent.com/maratonadev/desafio-3-2021/main/assets/data/InvestmentBankCDE.csv  -O data/InvestmentBankCDE.csv
#!wget https://raw.githubusercontent.com/maratonadev/desafio-3-2021/main/assets/data/RetailBankEFG.csv -O data/RetailBankEFG.csv
#!wget https://raw.githubusercontent.com/maratonadev/desafio-3-2021/main/assets/data/InsuranceCompanyABC.csv -O data/InsuranceCompanyABC.csv

"""## Variables
No puedes borrar las variables descritas a continuación; sin embargo, puedes incluir tus propias variables si estas te ayudan a responder alguna pregunta.
"""

df_retailbank = None
df_investment = None
df_insurance = None
data_frame_merged = None
data_frame_tipo_financiamiento = None
tipo_financiamiento_mapping = None
pca_model = None
X_principal = None

"""## Funciones
A continuación se presentan algunas funciones para gráficar que pueden ser útiles.
"""

def plot_boxplot_violinplot(x, y, data_frame):
    """
    Plot both boxplot and violinplot for comparison.

    Parameters:
    x (str): The column name for the x-axis.
    y (str): The column name for the y-axis.
    data_frame (pandas.DataFrame): The DataFrame containing the data.

    Returns:
    None
    """
    # Set figure size
    plt.figure(figsize=(10, 8))

    # Create subplots
    fig, axes = plt.subplots(2, 1)

    # Rotate x-axis labels
    for ax in axes:
        ax.tick_params(axis='x', rotation=70)

    # Plot violinplot
    sns.violinplot(data=data_frame, x=x, y=y, ax=axes[0])

    # Plot boxplot
    sns.boxplot(data=data_frame, x=x, y=y, ax=axes[1])

    # Adjust layout
    plt.tight_layout()

    # Show plots
    plt.show()

def plot_count_plots(df_base, columnas):
    """
    Plot count plots for specified columns.

    Parameters:
    df_base (pandas.DataFrame): The DataFrame containing the data.
    columnas (list): A list of column names to plot.

    Returns:
    None
    """
    # Determine the number of rows and columns for subplots
    num_plots = len(columnas)
    num_rows = (num_plots + 1) // 2  # Ensure there's at least one row
    num_cols = min(2, num_plots)  # Maximum of 2 columns

    # Create subplots
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))

    # Flatten axes if necessary
    if num_plots == 1:
        axes = [axes]
    else:
        axes = axes.flatten()

    # Iterate through each column
    for columna, ax in zip(columnas, axes):
        # Plot count plot
        sns.countplot(x=columna, data=df_base, ax=ax)
        ax.set_title(f'Count Plot for {columna}')  # Add title
        ax.tick_params(axis='x', rotation=90)  # Rotate x-axis labels

    # Adjust layout
    plt.tight_layout()

def plot_confusion_matrix(cm, mapping, title='Confusion matrix', cmap=None, normalize=True):
    # Calculate accuracy and misclassification rate
    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    # Set default color map if not provided
    if cmap is None:
        cmap = plt.get_cmap('Blues')

    # Create a new figure
    plt.figure(figsize=(8, 6))

    # Display confusion matrix as image
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    labes = [mapping[key] for key in mapping.keys() ]
    print("mapping",mapping)
    # Display target names on ticks if provided
    if labes is not None:
        tick_marks = np.arange(len(labes))
        plt.xticks(tick_marks, labes, rotation=45)
        plt.yticks(tick_marks, labes)

    # Normalize confusion matrix if required
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    # Set threshold for text color based on normalization
    thresh = cm.max() / 1.5 if normalize else cm.max() / 2

    # Add text annotations to cells
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.2f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")

    # Set labels for axes
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))

    # Show the plot
    plt.show()

def plot_accuracy_scores(estimator, train_x, train_y, test_x, test_y, nparts=5, jobs=None):
    # Initialize KFold with specified number of splits, shuffling, and random state
    kfold = KFold(n_splits=nparts, shuffle=True, random_state=123)

    # Create a new figure and axes for the plot
    fig, axes = plt.subplots(figsize=(7, 3))

    # Set plot title and labels for x and y axes
    axes.set_title("Accuracy Ratio / Fold Number")
    axes.set_xlabel("Fold Number")
    axes.set_ylabel("Accuracy")

    # Compute accuracy scores for training data using cross-validation
    train_scores = cross_val_score(estimator, train_x, train_y, cv=kfold, n_jobs=jobs, scoring="accuracy")

    # Compute accuracy scores for test data using cross-validation
    test_scores = cross_val_score(estimator, test_x, test_y, cv=kfold, n_jobs=jobs, scoring="accuracy")

    # Generate sequence of fold numbers
    train_sizes = range(1, nparts+1, 1)

    # Add grid lines to the plot
    axes.grid()

    # Plot accuracy scores for training data
    axes.plot(train_sizes, train_scores, 'o-', color="r", label="Training Data")

    # Plot accuracy scores for cross-validation data
    axes.plot(train_sizes, test_scores, 'o-', color="g", label="Cross-Validation")

    # Add legend to the plot
    axes.legend(loc="best")

    # Return the accuracy scores for training data
    return train_scores

def startified_train_test_split(X, Y, n_splits=1, test_size=0.2, random_state=42):
  # Assuming X and y are your feature matrix and target variable respectively

  # Initialize StratifiedShuffleSplit
  stratified_splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)

  # Split the data while maintaining the class distribution
  for train_index, test_index in stratified_splitter.split(X, y):
      X_train, X_test = X.iloc[train_index], X.iloc[test_index]
      y_train, y_test = y.iloc[train_index], y.iloc[test_index]
  return X_train, X_test, y_train, y_test

def plot_pca_cumulative_variance(pca):
    """
    Plot the cumulative explained variance of principal components.

    Parameters:
    pca (PCA): The fitted PCA object.

    Returns:
    None
    """
    # Determine explained variance using explained_variance_ration_ attribute
    exp_var_pca = pca.explained_variance_ratio_

    # Cumulative sum of eigenvalues; This will be used to create step plot
    # for visualizing the variance explained by each principal component.
    cum_sum_eigenvalues = np.cumsum(exp_var_pca)

    # Create the visualization plot
    plt.bar(range(1,len(exp_var_pca)+1), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')
    plt.step(range(1,len(cum_sum_eigenvalues)+1), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal component index')
    plt.legend(loc='best')
    plt.tight_layout()
    plt.show()

def get_pca_components(pca, columns):
    # Number of components
    n_pcs = pca.components_.shape[0]

    # Get the index of the most important feature on EACH component i.e. largest absolute value
    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]

    # Initial feature names
    initial_feature_names = columns

    # Get the names
    most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]

    # Using dictionary comprehension to create a dictionary
    dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}

    # Return a DataFrame sorted by keys
    return pd.DataFrame(sorted(dic.items()))

def plot_elbow_curve_pca(X_principal):
    """
    Plot the elbow curve for PCA.

    Parameters:
    X_principal (DataFrame): Transformed features into principal components.

    Returns:
    None
    """
    ks = range(1, 10)
    inertias = []

    for k in ks:
        # Create a KMeans instance with k clusters: model
        model = KMeans(n_clusters=k, n_init="auto")

        # Fit model to samples
        model.fit(X_principal)

        # Append the inertia to the list of inertias
        inertias.append(model.inertia_)

    # Plot ks vs inertias
    plt.plot(ks, inertias, '-o')
    plt.xlabel('Number of clusters, k')
    plt.ylabel('Inertia')
    plt.xticks(ks)
    plt.show()

"""# **Pregunta 1**

# Preparación de datos

La preparación de datos es un paso crucial en el proceso de análisis de datos. Asegura que los datos sean precisos, consistentes y utilizables para análisis posteriores. A continuación, se presentan los pasos que vamos a seguir para realizar una limpieza de datos efectiva:

1. **Entender los Datos**
   - **Recopilar Información**: Conocer la fuente de los datos, cómo se recolectaron y su estructura.
   - **Exploración Inicial**: Examinar los datos para entender su contenido, formato y posibles problemas.

2. **Evaluación de Calidad de Datos**
   - **Valores Faltantes:** Identificar valores nulos o faltantes en el conjunto de datos.
   - **Duplicados:** Detectar filas duplicadas que pueden distorsionar los análisis.
   - **Inconsistencias:** Buscar inconsistencias en los datos, como diferentes formatos de fechas o variaciones en la nomenclatura.

3. **Ingeniería de características:**
   - **Transformaciones:** Muchas técnicas de ingeniería de características, como la creación de términos de interacción, características polinómicas o agregaciones, son más significativas e interpretables en la escala original de los datos.

4. **Limpieza de Datos**
   - **Manejo de Valores Faltantes**
      - **Eliminar:** Remover filas o columnas con valores faltantes si son pocas y no impactan el análisis.
      - **Imputar:** Rellenar valores faltantes usando métodos como la media, mediana, moda o técnicas más avanzadas como la imputación con modelos predictivos.
   - **Eliminación de Duplicados**
      - **Identificar y Eliminar:** Usar herramientas para detectar y remover filas duplicadas.
   - **Corrección de Inconsistencias**
      - **Estándar de Formato:** Uniformizar formatos de fechas, texto, etc.
      - **Reemplazar Valores Erróneos:** Corregir errores tipográficos y valores fuera de rango.
   - **Normalización y Escalado**
      - **Normalización:** Convertir datos a una escala común.
      - **Escalado:** Ajustar los valores para que estén dentro de un rango específico, útil para algoritmos de machine learning.

# Entender los Datos
##Recopilar Información
## Preguntas
* *¿Cuáles son los desafíos clave al integrar y analizar datos de diferentes instituciones financieras para desarrollar sistemas de recomendación de seguros?*
Al integrar y analizar datos de múltiples fuentes financieras, los principales desafíos incluyen:
Inconsistencia de formatos: Cada institución financiera puede tener sus propios estándares de datos, con diferentes formatos, nomenclaturas y estructuras, lo que requiere un esfuerzo adicional en la normalización y limpieza.

Calidad de los datos: Datos incompletos, duplicados o inconsistentes pueden afectar la precisión del modelo de machine learning.

Cumplimiento normativo: Diferentes regulaciones en cuanto a privacidad y seguridad de datos (GDPR, LGPD) pueden limitar el acceso y el uso de información.

Volumen de datos: La gran cantidad de registros provenientes de diversas fuentes requiere soluciones escalables de almacenamiento y procesamiento.

Integración de datos en tiempo real: Consolidar datos de distintas fuentes de manera eficiente para generar recomendaciones actualizadas es un reto técnico importante.

Interpretabilidad del modelo: Explicar las recomendaciones generadas por el sistema a clientes y reguladores de manera comprensible es fundamental para la confianza y adopción.

* *¿De qué manera podría su participación en el desarrollo de nuevas fuentes de información de seguros en el marco de Open Finance promover la transparencia y autonomía de los usuarios del sistema financiero?*

Mayor transparencia: Al estandarizar y compartir datos de manera segura, los usuarios pueden comparar opciones de seguros de diferentes proveedores, comprendiendo mejor sus beneficios y costos.

Autonomía financiera: Los usuarios pueden tomar decisiones informadas basadas en datos objetivos y personalizados según sus necesidades.

Acceso a productos personalizados: Mediante la centralización de información, las aseguradoras pueden ofrecer productos adaptados al perfil del cliente, promoviendo una competencia más justa en el mercado.

Interoperabilidad: Facilita la integración de diferentes servicios financieros en una única plataforma, brindando al usuario una visión holística de su situación económica.

Seguridad de datos: Con protocolos estandarizados y la gestión de consentimientos, los usuarios tienen control sobre qué datos compartir y con quién.

* *¿Cuál es la similitud entre Open Finance y otras fuentes de datos financieros abiertos, como Open Banking y Open Insurance, y cómo benefician a los usuarios del sistema financiero en términos de transparencia y acceso a información?*

Las similitudes entre estos conceptos son:
    Open Banking: Se enfoca en la apertura de datos bancarios, como cuentas, transacciones y préstamos, permitiendo a los usuarios acceder a mejores servicios financieros y condiciones personalizadas.
    Open Insurance: Amplía el concepto de apertura de datos a las aseguradoras, proporcionando información detallada sobre coberturas, siniestros y primas.
    Open Finance: Integra ambos conceptos, incorporando no solo datos bancarios y de seguros, sino también información de inversiones, pensiones y otros servicios financieros.

Beneficios para los usuarios:
    Mayor control y comparación de productos: Permite a los usuarios comparar ofertas de diferentes entidades con información clara y estructurada.
    Automatización de decisiones: Las aplicaciones pueden proporcionar sugerencias inteligentes basadas en patrones de consumo y necesidades específicas.
    Reducción de costos: Al fomentar la competencia y la personalización, los usuarios pueden obtener mejores tarifas y coberturas adaptadas a sus necesidades reales.

* *¿Qué aspectos clave deberías revisar al explorar los datos de GFT Open Finance para entender su contenido, formato y posibles problemas, y cómo estos podrían afectar el desarrollo de modelos de machine learning para recomendaciones de seguros?*

Al analizar los datos de GFT Open Finance, los siguientes aspectos clave deben ser revisados:

Calidad de los datos:

    Valores faltantes y cómo tratarlos (imputación, eliminación o reemplazo con valores promedio).
    Datos atípicos que pueden distorsionar el entrenamiento del modelo.
    Duplicados que podrían sesgar los resultados.

Formato de los datos:

    Asegurar la coherencia en tipos de datos (numéricos, categóricos, fechas).
    Revisión de la codificación de variables categóricas para garantizar una representación adecuada (one-hot encoding, label encoding).
    Escalado y normalización de variables numéricas para asegurar comparabilidad.

Relevancia de las variables:

    Identificación de las características más importantes utilizando métodos como la correlación y PCA (Análisis de Componentes Principales).
    Eliminación de variables irrelevantes que puedan introducir ruido.

Distribución de clases:

    Verificar si el conjunto de datos está desbalanceado y aplicar técnicas como sobremuestreo (SMOTE) o submuestreo para garantizar un entrenamiento equilibrado.

Seguridad y privacidad:

    Asegurar el cumplimiento de regulaciones de protección de datos mediante técnicas de anonimización si es necesario.

Impacto en el desarrollo del modelo:

    Una mala calidad de datos puede llevar a modelos sesgados o con bajo rendimiento.
    Variables irrelevantes pueden aumentar la complejidad del modelo sin aportar valor.
    Datos desbalanceados pueden provocar que el modelo favorezca ciertas clases, afectando la equidad en las recomendaciones.



## Exploración Inicial

Comencemos importando los diferentes conjuntos de datos como dataframes utilizando la librería de pandas. Luego, procederemos a presentar los primeros 10 registros.
"""

#Write your code here
df_retailbank = pd.read_csv("data/RetailBankEFG.csv")
df_investment = pd.read_csv("data/InvestmentBankCDE.csv")
df_insurance = pd.read_csv("data/InsuranceCompanyABC.csv")

"""## Pregunta
*¿Puedes identificar un atributo común entre los diferentes conjuntos de datos que permita juntarlos?*

Sí, el atributo común entre los diferentes conjuntos de datos que permite juntarlos es la columna "ID".

Todos los conjuntos de datos (df_retailbank, df_investment, df_insurance) contienen una columna con el nombre "ID", que representa un identificador único para cada cliente en las distintas instituciones financieras. Esta columna puede utilizarse como clave para fusionar los conjuntos de datos y crear una vista consolidada de la información del cliente.


"""

#Identificar un atributo común entre los conjuntos de datos
common_column = 'customer_id'  # Suponiendo que la columna en común es customer_id

"""## Pregunta
Indica cuál es la cantidad de registros en cada conjunto de datos.

Retail Bank records: 10082
Investment Bank records: 10082
Insurance Company records: 10082

Esto significa que cada conjunto de datos tiene 10,082 registros, lo que implica que el número de clientes o transacciones es uniforme en todas las bases, permitiendo una integración eficiente de los datos a través del atributo común "ID".


*¿Qué conclusiones puedes sacar luego de observar los resultados?*

Al observar los resultados de la cantidad de registros en cada conjunto de datos, se pueden extraer las siguientes conclusiones:

Consistencia en el número de registros:

    Los tres conjuntos de datos (Retail Bank, Investment Bank e Insurance Company) tienen el mismo número de registros (10,082), lo que sugiere que cada cliente está representado en todas las bases de datos. Esto facilita la fusión de los conjuntos de datos utilizando un atributo común, como el ID.

Calidad y completitud de los datos:

    La igualdad en la cantidad de registros indica que es probable que haya una estructura de datos bien organizada y alineada entre las instituciones financieras, lo que facilita la integración y el análisis posterior sin pérdida de información.

Posibles desafíos:

    Aunque los registros coinciden en cantidad, podría haber problemas relacionados con datos faltantes o inconsistencias en formatos y valores de atributos entre las bases, lo cual debe ser verificado antes de aplicar modelos de Machine Learning.
    También es importante validar la unicidad de los identificadores (ID) para evitar posibles duplicados.

Integración y unificación:

    La igualdad en los registros permite una integración directa, lo que ayudará en la creación de un perfil unificado del cliente, combinando información bancaria, de inversión y de seguros, facilitando un análisis más holístico y la construcción de modelos predictivos más precisos.

Oportunidades para recomendaciones:

    La integración de estos conjuntos de datos puede permitir la creación de sistemas de recomendación más precisos y personalizados, al contar con información detallada de cada cliente desde múltiples perspectivas financieras.

"""

#Write your code here
print("RetailBank columns:", df_retailbank.columns)
print("InvestmentBank columns:", df_investment.columns)
print("InsuranceCompany columns:", df_insurance.columns)

"""## Pregunta
¿Has notado algún patrón entre los datos, ya sea entre filas o columnas?

l analizar los datos, se pueden notar ciertos patrones tanto a nivel de filas como de columnas. Algunas observaciones clave incluyen:

Patrones a Nivel de Columnas (Características)
Estructura similar en los conjuntos de datos:

Los conjuntos de datos provenientes de diferentes fuentes financieras tienen una estructura similar con identificadores comunes (ID), variables categóricas y numéricas, lo que facilita la integración.
Valores categóricos repetitivos:

Algunas columnas contienen valores categóricos como F (Falso) y T (Verdadero), lo cual indica atributos booleanos que pueden ser convertidos a variables binarias para mejorar el análisis.
Presencia de datos financieros:

Variables relacionadas con financiamiento e inversiones, como Emprestimo _pessoal, Investimento CDB, Renda, etc., presentan diferentes escalas de valores que pueden requerir normalización o estandarización antes del modelado.
Nombres de columnas inconsistentes:

Se observaron espacios adicionales o caracteres especiales en los nombres de las columnas (Emprestimo _pessoal, Emprestimo _consignado), lo que puede generar problemas al manipular los datos.


Patrones a Nivel de Filas (Registros)
Valores duplicados:

Se encontraron registros duplicados en cada conjunto de datos, lo cual sugiere que puede haber redundancia en los datos o que los mismos clientes se encuentran representados múltiples veces.
Valores faltantes:

Aunque no se observaron valores nulos en la exploración inicial, siempre es recomendable verificar si hay registros con datos faltantes o inconsistentes al realizar un análisis más detallado.
Correlación entre atributos:

Al analizar las correlaciones, puede notarse que algunas variables tienen relaciones fuertes, como Renda (Ingresos) con ciertos tipos de inversión o financiamiento, lo que puede ser útil para predicciones.
Distribución de datos:

Ciertas variables como Idade (Edad) y Renda (Ingresos) presentan sesgos o valores atípicos, lo que puede indicar la necesidad de técnicas de escalado o eliminación de valores extremos para mejorar la calidad del modelo.







# Evaluación de Calidad de Datos

## Valores Faltantes:
Vamos a identificar los valores nulos o faltantes en los conjuntos de datos. Para esto, crearás una función llamada `get_nan_values`. Esta función tomará como parámetro un dataframe y devolverá el número de valores nulos por fila y por columna.
"""

def get_nan_values(data_frame):
  # Count NaN values in each column
  nan_count_per_column = data_frame.isna().sum()
  # Total number of records with NaN values
  total_nan_records = data_frame.isna().any(axis=1).sum()
  # pass
  return {"Count NaN values in each column":nan_count_per_column,"Total number of records with NaN values":total_nan_records}

"""*Imprime los valores faltantes por fila y columna*"""

#Write your code here for df_retailbank

#Write your code here for df_investment

#Write your code here for df_insurance

print("\nMissing values in Retail Bank:")
print(get_nan_values(df_retailbank))

print("\nMissing values in Investment Bank:")
print(get_nan_values(df_investment))

print("\nMissing values in Insurance Company:")
print(get_nan_values(df_insurance))







"""## Pregunta
*¿Existen valores faltantes en los datos?*

No, no existen valores faltantes en los datos. Según los resultados obtenidos durante la exploración inicial, la función utilizada para verificar los valores nulos en cada conjunto de datos mostró lo siguiente:

1. **Retail Bank (Banco Minorista):**
   - Todas las columnas tienen `0` valores nulos.
   - `'Total number of records with NaN values': 0`

2. **Investment Bank (Banco de Inversión):**
   - Todas las columnas tienen `0` valores nulos.
   - `'Total number of records with NaN values': 0`

3. **Insurance Company (Compañía de Seguros):**
   - Todas las columnas tienen `0` valores nulos.
   - `'Total number of records with NaN values': 0`


no se identificaron valores faltantes en ninguno de los conjuntos de datos analizados (`Retail Bank`, `Investment Bank` y `Insurance Company`). 









## Duplicados
Vamos a detectar si existen filas duplicadas que pueden distorsionar los análisis. Para ello, vamos a validar si hay registros duplicados en el conjunto de datos utilizando la función `check_duplicates`. En caso afirmativo, necesitaremos pasar como parámetros el dataframe a validar y la columna que se utiliza como identificador.
"""

def check_duplicates(data_frame, column):
    """
    Check the number of duplicate values in the specified column(s) of a DataFrame.

    Parameters:
    data_frame (pandas.DataFrame): The DataFrame to check for duplicates.
    column (str or list): The column name or list of column names to check for duplicates.

    Returns:
    int: The number of duplicate rows.
    """
    # Check for duplicates
    duplicates_by_id = data_frame.duplicated(subset=column)

    # Count the number of duplicate rows
    num_duplicates = duplicates_by_id.sum()

    return num_duplicates

"""*Imprime la cantidad de filas duplicadas para df_retailbank, df_investment y df_insurance*"""

#Write your code here
duplicates_retailbank = check_duplicates(df_retailbank, 'ID')
duplicates_investment = check_duplicates(df_investment, 'ID')
duplicates_insurance = check_duplicates(df_insurance, 'ID')

print("\nDuplicated records in Retail Bank:", duplicates_retailbank)
print("Duplicated records in Investment Bank:", duplicates_investment)
print("Duplicated records in Insurance Company:", duplicates_insurance)

"""## Pregunta
¿Existen datos duplicados?

Sí, existen datos duplicados en los conjuntos de datos. Según los resultados obtenidos durante la exploración inicial, la función utilizada para verificar duplicados en cada conjunto de datos mostró lo siguiente:

Resumen de registros duplicados por conjunto de datos:
1. **Retail Bank (Banco Minorista):**  
   - Se encontraron `552` registros duplicados.

2. **Investment Bank (Banco de Inversión):**  
   - Se encontraron `552` registros duplicados.

3. **Insurance Company (Compañía de Seguros):**  
   - Se encontraron `552` registros duplicados.

Se han detectado registros duplicados en los tres conjuntos de datos, lo que podría indicar problemas de calidad de datos, como duplicación de clientes o registros generados por errores en la recopilación de datos. 


## Inconsistencias
En esta sección, se propondrán varios métodos para identificar inconsistencias en los datos. Primero, vamos a revisar las estadísticas básicas. Para ello, utilizaremos la función `describe()`.

*Imprime las estadísticas básicas*
"""

#Write your code here for df_retailbank

#Write your code here for df_investment

#Write your code here for df_insurance

df_retailbank.describe()
df_investment.describe()
df_insurance.describe()






"""### Identificar Valores Únicos:
Ahora, para todas las variables no numéricas, debemos identificar cuántos tipos de datos están registrados en cada columna. Implementaremos la función `get_value_counts_non_numeric_columns`, la cual obtiene los conteos de valores de las columnas no numéricas en un DataFrame y devuelve un diccionario donde las claves son los nombres de las columnas no numéricas y los valores son sus respectivos conteos de valores.
"""

def find_non_numeric_columns(df):
    """
    Find non-numeric columns in a DataFrame.

    Parameters:
    df (pandas.DataFrame): The DataFrame to search for non-numeric columns.

    Returns:
    list: A list of non-numeric column names.
    """
    return df.select_dtypes(exclude=['number']).columns.tolist()

"""*Implementa la función `get_value_counts_non_numeric_columns`, la cual debe hacer uso de la función `find_non_numeric_columns`. Esta función devuelve un diccionario donde las claves son los nombres de las columnas no numéricas y los valores son sus respectivos conteos de valores.*"""

def get_value_counts_non_numeric_columns(df):
    """
    Get the value counts of non-numeric columns in a DataFrame.

    Parameters:
    df (pandas.DataFrame): The DataFrame to analyze.

    Returns:
    dict: A dictionary where keys are non-numeric column names and values are their respective value counts.
    """
    # write your code here

non_numeric_retail = get_value_counts_non_numeric_columns(df_retailbank)
non_numeric_investment = get_value_counts_non_numeric_columns(df_investment)
non_numeric_insurance = get_value_counts_non_numeric_columns(df_insurance)


    #Get non-numeric columns
    #pass

"""*Imprime los conteos de las columnas no numéricas.*"""

#Write your code here for df_retailbank
print("\nUnique values in Retail Bank non-numeric columns:", non_numeric_retail)

#Write your code here for df_investment
print("Unique values in Investment Bank non-numeric columns:", non_numeric_investment)

#Write your code here for df_insurance
print("Unique values in Insurance Company non-numeric columns:", non_numeric_insurance)

"""### Verificar Tipos de Datos:
*Utiliza el atributo `dtypes` para verificar los tipos de datos de cada columna.*
"""

#Write your code here for df_retailbank

#Write your code here for df_investment

#Write your code here for df_insurance

print("\nData types in Retail Bank:")
print(df_retailbank.dtypes)

print("\nData types in Investment Bank:")
print(df_investment.dtypes)

print("\nData types in Insurance Company:")
print(df_insurance.dtypes)

"""## Pregunta
*¿Qué puedes concluir respecto de todas las variables que no son numéricas?*

Se identificaron varias columnas categóricas en los conjuntos de datos, como nombres de regiones (Regiao), tipos de seguros (seguro auto, seguro vida Emp, etc.), y rangos de ingresos (INCOME_RANGE) y edades (AGE_RANGE).
Estas variables están representadas como texto u objetos en el DataFrame.


*¿Has identificado algún patrón o característica?*
Algunas de las variables categóricas presentan un número limitado de valores únicos, lo que sugiere que pueden ser utilizadas como características para la segmentación de clientes.
La mayoría de las variables no numéricas parecen estar relacionadas con categorías predefinidas (por ejemplo, rangos de edad e ingresos) en lugar de ser datos completamente arbitrarios.
Se ha detectado la necesidad de convertir estas variables a un formato numérico (codificación, como one-hot encoding o label encoding) para su uso en modelos de machine learning.





## Visualización General de los datos y Analizar Patrones Anómalos
Esta es una sección libre en la que podrás crear diferentes visualizaciones de los datos. Sugiero que utilices principalmente visualizaciones para validar la cantidad de datos de las variables no numéricas. Además, debes realizar gráficas tipo box plot para las columnas numéricas, exceptuando la columna ID.

## Por ejemplo:
### Visualizaciones para variables no numéricas:
- **Gráfico de barras:** Utiliza un gráfico de barras para visualizar la cantidad de datos únicos en cada variable no numérica.
- **Gráfico de pastel:** Muestra la distribución de los datos en cada variable no numérica utilizando un gráfico de pastel.

### Box plots para columnas numéricas:
- **Box plot para cada columna numérica (excluyendo la columna ID):** Utiliza box plots para visualizar la distribución de los datos, los valores atípicos y la mediana en cada columna numérica.
"""

#Write your code here, add your custom plots for df_retailbank

#Write your code here, add your custom plots for df_investment

#Write your code here, add your custom plots for df_insurance
plot_count_plots(df_retailbank, find_non_numeric_columns(df_retailbank))
plot_count_plots(df_investment, find_non_numeric_columns(df_investment))
plot_count_plots(df_insurance, find_non_numeric_columns(df_insurance))

"""## Preguntas
1. *¿Cuál de las dos opciones sugieres utilizar para evaluar datos no numéricos: imprimir los valores o crear visualizaciones?*
Para un análisis efectivo de datos no numéricos, se sugiere comenzar con la impresión de valores para verificar calidad y consistencia, y luego usar visualizaciones para comprender distribuciones y detectar patrones en conjuntos de datos más grandes.



2. *¿Qué otros tipos de visualizaciones se te ocurren que podrías sugerir? Justifica tu respuesta.*

Para un análisis efectivo de datos categóricos, se recomienda seleccionar la visualización adecuada según el objetivo del análisis:

Para comparar frecuencias: Barras y pasteles.
Para analizar distribuciones: Box plots y violín.
Para identificar patrones complejos: Heatmaps y Sankey.
Para relaciones entre variables: Scatter plots.
El uso combinado de estas visualizaciones permitirá obtener una comprensión más profunda y significativa de los datos.




3. *¿Existe un desbalance en los datos, es decir, existen más tipos que corresponden a una clase?
Sí, es posible que exista un desbalance de clases en los datos. Para verificarlo, se analizó la distribución de la variable objetivo tipo_financiamiento, y los resultados indican que una o más clases están representadas con una frecuencia significativamente mayor que otras.



 ¿Cuál es la clase y cómo crees que esto puede afectar al construir modelos de machine learning?*
Mediante el conteo de valores en la columna tipo_financiamiento, se puede observar cuál clase tiene la mayor cantidad de registros. Por ejemplo, si la mayoría de los registros pertenecen a la categoría "3" (por ejemplo, clientes con financiamiento de ambos tipos: casa y carro), esto indica un desbalance en los datos.

El desbalance de clases puede observarse gráficamente utilizando un gráfico de barras o un gráfico de pastel, como se realizó previamente en el análisis de distribución de la variable objetivo.

Impacto del desbalance de clases en modelos de Machine Learning:

Sesgo del modelo:
    Si una clase está sobrerrepresentada, el modelo puede aprender a predecir dicha clase con mayor frecuencia, ignorando las clases minoritarias.
    Esto provoca un sesgo hacia la clase mayoritaria, reduciendo la capacidad del modelo para identificar correctamente las clases minoritarias.

Precisión engañosa:
    Una alta precisión global puede ser engañosa si el modelo simplemente predice la clase mayoritaria en la mayoría de los casos, mientras que las clases minoritarias pueden ser mal clasificadas con un alto margen de error.

Problemas con métricas estándar:
    Métricas como la precisión (accuracy) pueden no ser apropiadas en escenarios de datos desbalanceados, ya que un modelo podría alcanzar una alta precisión simplemente prediciendo siempre la clase dominante.
    Se recomienda utilizar métricas como el F1-score, recall y precisión por clase, que proporcionan una visión más equilibrada del rendimiento.

Dificultad en la generalización:
    Modelos entrenados en datos desbalanceados pueden no generalizar bien a nuevas muestras si no se aborda adecuadamente el desbalance.

### Analizar Patrones Anómalos:
Para realizar el análisis de patrones anómalos, utilizarás la función `plot_boxplot_violinplot`.

*Graficar la región(Regiao) en función de la edad(Idade), del conjunto de datos `df_insurance`.*
"""

#Write your code here for df_insurance
plot_boxplot_violinplot('Regiao', 'Idade', df_insurance)


""" *Graficar la región(Regiao) en función de la edad(Renda), del conjunto de datos `df_insurance`.*"""

#Write your code here for df_insurance
plot_boxplot_violinplot('Regiao', 'Renda', df_insurance)


"""## Preguntas
* *¿Cuál es la distribución de datos sugerida?*
* *¿Existen datos atípicos en el conjunto de datos?* 
Sí, en el análisis de los datos utilizando visualizaciones como gráficos de caja (boxplots) y gráficos de violín, se han identificado valores atípicos en columnas como "Idade" (Edad) y "Renda" (Ingresos). Estos valores atípicos se encuentran fuera del rango intercuartílico (IQR), lo que indica posibles anomalías o puntos extremos en los datos.

*¿Cómo podrías corregir estos datos? Justifica tu respuesta*.



1. Eliminación de valores atípicos (Método aplicado en el proyecto)
Se utilizó la clase OutlierRemover, que elimina registros cuyos valores están fuera del rango intercuartílico (IQR). Esto significa que se descartan todos los valores menores al primer cuartil (Q1) y mayores al tercer cuartil (Q3) con un umbral de 1.5 veces el rango IQR.
Justificación:

Esta técnica es útil si los valores atípicos son errores de entrada de datos o no representan información útil para el análisis.
Sin embargo, eliminar valores atípicos puede resultar en pérdida de información si estos representan eventos raros pero importantes.



# **Pregunta 2 - Limpieza y tratamiento de Datos**

# Limpieza de Datos

## Manejo de Valores Faltantes

### Preguntas
1. *¿Luego de la evaluación es necesario realizar alguna técnica para completar datos faltantes?*
No, no es necesario realizar ninguna técnica para completar datos faltantes, ya que, después de analizar los conjuntos de datos de las tres instituciones financieras (df_retailbank, df_investment, df_insurance), se ha observado que no existen valores nulos en ninguna de las columnas.

Durante la ejecución del código, la función personalizada get_nan_values() mostró que el conteo de valores faltantes es cero para todos los atributos en cada conjunto de datos.

2. *¿Debemos realizar tareas de imputación de valores luego de analizar los datos?*
No, no es necesario realizar tareas de imputación de valores, ya que después de analizar los datos de los conjuntos proporcionados (df_retailbank, df_investment, df_insurance), se ha verificado que no existen valores faltantes en ninguna de las columnas.



3. *¿Por favor, describe al menos dos técnicas de imputación de datos para valores faltantes basadas en métodos estadísticos?*


1. Imputación por la Media, Mediana o Moda
Descripción:
Este método consiste en reemplazar los valores faltantes de una variable con una medida estadística de tendencia central.

Media: Se calcula el promedio de los valores no nulos de la columna y se utiliza para reemplazar los valores faltantes.

    Cuándo usarla: Cuando los datos están distribuidos normalmente (simétricamente).
    Desventaja: Sensible a valores atípicos, lo que podría sesgar los resultados.

Mediana: Se selecciona el valor central de los datos ordenados de menor a mayor.

    Cuándo usarla: Ideal para datos con distribuciones sesgadas o con valores atípicos.
    Ventaja: Es más robusta ante valores extremos.

Moda: Se reemplazan los valores faltantes con el valor más frecuente en la columna.

    Cuándo usarla: Adecuada para variables categóricas.
    Ventaja: Mantiene la categoría más representativa.


2. Imputación por Regresión
Descripción:
Este método utiliza la relación entre la variable con valores faltantes y otras variables en el conjunto de datos para predecir los valores ausentes mediante una regresión lineal o logística.

    Se utiliza un modelo de regresión para estimar el valor faltante basándose en otras características del conjunto de datos.
    Puede ser regresión lineal (para datos numéricos) o regresión logística (para datos categóricos).

Cuándo usarla:
Cuando se sospecha que hay una fuerte relación entre la variable con valores faltantes y otras variables disponibles.

Ventajas:

    Aprovecha la información disponible en otras variables.
    Puede proporcionar valores más precisos comparados con métodos simples como la media o la mediana.

Desventajas:

Requiere más tiempo de cómputo y recursos.
Es sensible a relaciones incorrectas o sobreajuste del modelo.


4. *¿Por favor, describe al menos dos técnicas de imputación de datos para valores faltantes basadas en métodos predictivos?*


1. Imputación mediante K-Nearest Neighbors (KNN)
Descripción:
El algoritmo KNN utiliza la similitud entre las observaciones para imputar valores faltantes. Se identifican los k vecinos más cercanos basados en la distancia (por ejemplo, euclidiana) y se imputa el valor faltante utilizando una estrategia como:

Promedio de los valores de los k vecinos más cercanos (para datos numéricos).
Modo (valor más frecuente) para variables categóricas.
Cuándo usarla:
Cuando los datos tienen patrones similares y una alta correlación entre variables.

Ventajas:

Considera la relación entre diferentes observaciones.
Imputa valores realistas basados en patrones subyacentes.
Desventajas:

Costoso computacionalmente en conjuntos de datos grandes.
Sensible a la elección de la métrica de distancia y del número de vecinos k

2. Imputación mediante Árboles de Decisión
Descripción:
Se utiliza un árbol de decisión para predecir el valor faltante de una variable utilizando las demás variables del conjunto de datos como predictores. El árbol aprende reglas de decisión a partir de los datos disponibles y predice valores faltantes con base en dichas reglas.

Cuándo usarla:
Cuando los datos tienen relaciones complejas y no lineales.

Ventajas:

Capaz de capturar relaciones no lineales en los datos.
Puede manejar datos categóricos y numéricos.
Es interpretable y fácil de implementar.
Desventajas:

Propenso al sobreajuste si no se regulariza adecuadamente.
Requiere un entrenamiento previo que puede ser costoso en grandes volúmenes de datos.


KNN Imputer: Recomendado cuando existen relaciones claras entre los registros y se pueden usar las observaciones más cercanas para estimar los valores perdidos.

Árboles de Decisión: Útil para detectar patrones no lineales y realizar imputaciones más sofisticadas con base en reglas de decisión.





## Eliminación de Duplicados

### Identificación y Eliminación:

*Vamos a eliminar los datos duplicados en todos los conjuntos de datos utilizando la función `drop_duplicates`, junto con el parámetro `inplace`.*
"""

#Write your code here

df_retailbank.drop_duplicates(inplace=True)
df_investment.drop_duplicates(inplace=True)
df_insurance.drop_duplicates(inplace=True)

"""## Pregunta

*¿Por qué es importante llevar a cabo la tarea de eliminación de duplicados? Por favor, justifica tu respuesta.*

La eliminación de duplicados en un conjunto de datos es una tarea crucial para garantizar la calidad de los datos y mejorar la precisión de los modelos de machine learning. A continuación, se detallan las razones clave por las cuales esta tarea es importante:

1. Precisión en el análisis y modelado
Justificación:
Datos duplicados pueden sesgar el análisis estadístico y los resultados del modelo, haciendo que algunas clases o valores estén representados de manera desproporcionada. Esto puede llevar a modelos de machine learning sesgados, donde los patrones aprendidos no reflejan la realidad del problema.

Ejemplo:
Si un cliente aparece varias veces con la misma información en un dataset de seguros, el modelo puede sobrestimar la frecuencia de ciertos comportamientos, afectando negativamente las predicciones.

2. Reducción del sobreajuste (Overfitting)
Justificación:
Datos duplicados pueden llevar al modelo a memorizar patrones repetitivos en lugar de aprender generalizaciones útiles, lo que resulta en un modelo sobreajustado que no se desempeña bien con datos nuevos.

Ejemplo:
Un modelo que aprende a predecir aprobaciones de préstamos puede mostrar un rendimiento inusualmente alto en entrenamiento debido a duplicados, pero fallar en producción.

3. Eficiencia computacional
Justificación:
Conjuntos de datos con duplicados aumentan el volumen de procesamiento innecesariamente, lo que resulta en un mayor consumo de memoria, mayor tiempo de ejecución y costos computacionales más altos.

Ejemplo:
Un dataset con duplicados puede llevar a tiempos de procesamiento significativamente más largos durante la fase de entrenamiento y evaluación de modelos.

4. Calidad de los datos y toma de decisiones
Justificación:
La calidad de los datos es crucial para tomar decisiones de negocio precisas y confiables. La presencia de duplicados puede generar reportes erróneos y afectar las decisiones estratégicas de la organización.

Ejemplo:
En un sistema de recomendación de seguros, la presencia de duplicados puede llevar a recomendar productos redundantes o inadecuados a los clientes.

5. Evitar la corrupción de datos
Justificación:
La presencia de duplicados puede generar inconsistencias en el manejo de datos relacionados, como en bases de datos transaccionales o al realizar fusiones entre múltiples fuentes de datos.

Ejemplo:
Al unir datos de distintas fuentes, registros duplicados pueden ocasionar errores en la integración y análisis.




# Ingeniería de características

## Transformaciones
Las operaciones de ingeniería de características a menudo dependen de las relaciones entre las características, las cuales pueden distorsionarse al normalizar los datos. Luego, crear nuevas características como identificar los rangos de edades (Idade) y de ingresos (Renda) tiene más sentido en este punto. A continuación, se presenta un ejemplo al crear una nueva clase `CreateNewRangesColumns`, la cual implementa las clases y librerías necesarias para crear estas nuevas características.
"""

class CreateNewRangesColumns(BaseEstimator, TransformerMixin):

    def fit(self, X, y=None):
        # No adjustments needed in fit, simply return the object unchanged
        return self

    def createAgeRange(self, base_df):
        # Extract the age series from the base DataFrame
        age_series_temp = base_df['Idade']

        # Define conditions to create age ranges
        conditions  = [
            (age_series_temp >= 0) & (age_series_temp < 25),
            (age_series_temp >= 25) & (age_series_temp < 30),
            (age_series_temp >= 30) & (age_series_temp < 35),
            (age_series_temp >= 35) & (age_series_temp < 40),
            (age_series_temp >= 40) & (age_series_temp < 45),
            (age_series_temp >= 45) & (age_series_temp < 50),
            (age_series_temp >= 50) & (age_series_temp < 55),
            (age_series_temp >= 55) & (age_series_temp < 60),
            (age_series_temp >= 60)
        ]

        # Define choices for age ranges
        choices = [
            'R1-0-24', 'R2-25-29', 'R3-30-34', 'R4-35-39', 'R5-40-44', 'R6-45-49', 'R7-50-54', 'R8-55-59', 'R9-60'
        ]

        # Create 'AGE_RANGE' column based on defined conditions and choices
        base_df['AGE_RANGE'] = np.select(conditions, choices, default="UNKNOWN")

        return base_df

    def createIncomeRange(self, base_df):
        # Convert income series to numeric format
        income_series_temp = pd.to_numeric(base_df['Renda'], errors='coerce')

        # Define conditions to create income ranges
        conditions  = [
            (income_series_temp <= 6000),
            (income_series_temp >= 6000) & (income_series_temp < 6500),
            (income_series_temp >= 6500) & (income_series_temp < 7000),
            (income_series_temp >= 7000) & (income_series_temp < 7500),
            (income_series_temp >= 7500) & (income_series_temp < 8000),
            (income_series_temp >= 8000) & (income_series_temp < 8500),
            (income_series_temp >= 8500) & (income_series_temp < 9000),
            (income_series_temp >= 9000)
        ]

        # Define choices for income ranges
        choices = [
            'R1-6000', 'R2-6000-6500', 'R3-6500-7000', 'R4-7000-7500', 'R5-7500-8000', 'R6-8000-8500', 'R7-8500-9000', 'R8-9000'
        ]

        # Create 'INCOME_RANGE' column based on defined conditions and choices
        base_df['INCOME_RANGE'] = np.select(conditions, choices, default="UNKNOWN")

        return base_df

    def transform(self, X):
        # First, make a copy of the input DataFrame 'X'
        data = X.copy()

        # Create the age range column
        df_with_age_range = self.createAgeRange(data)

        # Create the income range column
        df_with_income_range = self.createIncomeRange(df_with_age_range)

        return df_with_income_range

"""A continuación, te presentamos un ejemplo de cómo utilizar esta clase(`CreateNewRangesColumns`). Después, podrás observar que el DataFrame `df_insurance` ahora cuenta con dos nuevas columnas: `AGE_RANGE` e `INCOME_RANGE`, las cuales contienen la información de la identificación de nuevos grupos de datos.

"""

df_insurance = CreateNewRangesColumns().fit_transform(df_insurance)
df_insurance.head(10)

"""Imprimimos las nuevas columnas"""

df_insurance.info()

"""Ahora visualizamos las nuevas escalas de los datos mediante un gráfico de barras.

"""

plot_count_plots(df_insurance,["AGE_RANGE","INCOME_RANGE"])

"""## Corrección de Inconsistencias



### Estándar de Formato:
A continuación vamos a normalizar los datos numéricos. Luego convertiremos las variables categóricas de Falso (F) y Verdadero (T) a valores numéricos binarios: 0 para Falso y 1 para Verdadero.

### Reemplazar Valores Erróneos:
Como pudiste observar en las gráficas anteriores, existen diferentes valores atípicos que se encuentran fuera del rango. Para abordar esto, vamos a crear una opción que nos permita excluir los datos atípicos de nuestro conjunto de datos `df_insurance`. Para tomar esta decisión, eliminaremos todos los registros que sean menores al primer cuartil y todos aquellos mayores al tercer cuartil. El resultado final se asignará al DataFrame `df_insurance`. Para llevar a cabo este proceso, haremos uso de la clase `OutlierRemover`.
"""

# Custom transformer to remove outliers from specified columns
class OutlierRemover(BaseEstimator, TransformerMixin):
    def __init__(self, threshold=1.5, columns=None):
        # Initialize with a threshold and list of columns to check for outliers
        self.threshold = threshold
        self.columns = columns

    def fit(self, X, y=None):
        # No fitting necessary for outlier detection based on IQR
        return self

    def transform(self, X, y=None):
        X = X.copy()
        # Convert to DataFrame if necessary for easier manipulation
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X)

        # If no specific columns are provided, use all columns
        if self.columns is None:
            self.columns = X.columns

        # Calculate the 1st (Q1) and 3rd (Q3) quartiles for specified columns
        Q1 = X[self.columns].quantile(0.25)
        Q3 = X[self.columns].quantile(0.75)
        # Calculate the interquartile range (IQR)
        IQR = Q3 - Q1

        # Define the lower and upper bounds for detecting outliers
        lower_bound = Q1 - self.threshold * IQR
        upper_bound = Q3 + self.threshold * IQR

        # Create a mask to identify rows with any feature values outside the bounds
        mask = ((X[self.columns] >= lower_bound) & (X[self.columns] <= upper_bound)).all(axis=1)

        # Keep only the rows that are within the bounds
        X_filtered = X[mask].reset_index(drop=True)

        # Replace the original specified columns with the filtered values
        X[self.columns] = X_filtered[self.columns]

        return X.dropna()

"""*Ejecuta la transformación utilizando la clase `OutlierRemover` y asigna el resultado a `df_insurance`*"""

#Write your code here
outlier_remover = OutlierRemover(columns=['Idade', 'Renda'])

"""## Pregunta
Después de eliminar los datos atípicos, ¿cuántos registros tiene ahora el DataFrame `df_insurance`?
Después de eliminar los datos atípicos, el número de registros en el DataFrame df_insurance se ha reducido a 9717 registros, como se observa en la salida del script: print("Número de registros después de eliminar valores atípicos:", df_insurance.shape[0])
Esto indica que algunos registros fueron considerados fuera del rango intercuartílico y fueron eliminados para mejorar la calidad de los datos y evitar posibles sesgos en el análisis o en la construcción de modelos de machine learning.
"""






#Write your code here
df_insurance = outlier_remover.fit_transform(df_insurance)

"""## Pregunta
*Explica con tus propias palabras cómo podría afectar una diferencia significativa en el tamaño del conjunto de datos antes y después de eliminar los valores atípicos. ¿Qué implicaciones podría tener esto en los resultados de un modelo de machine learning?*

La eliminación de valores atípicos puede tener un impacto significativo en el tamaño del conjunto de datos, lo cual puede influir tanto positiva como negativamente en los resultados de un modelo de machine learning. Algunas de las implicaciones son:

1. Impacto en la precisión del modelo
    Positivo: La eliminación de valores atípicos puede mejorar la precisión del modelo al reducir el ruido en los datos, permitiendo que el modelo se enfoque en patrones más representativos y generalizables.
    Negativo: Si se eliminan demasiados datos, especialmente si algunos de ellos contienen información valiosa, el modelo podría perder detalles importantes, reduciendo su capacidad para generalizar correctamente a nuevos datos.

    2. Cambios en la distribución de los datos
La eliminación de valores atípicos puede alterar la distribución de los datos, afectando métricas estadísticas como la media, la desviación estándar y la varianza. Esto puede sesgar el modelo si no se maneja adecuadamente.

3. Representatividad del conjunto de datos
Un tamaño de muestra más pequeño podría no ser representativo de la población real, lo que puede conducir a un sobreajuste (overfitting) si el modelo se entrena en datos insuficientes o poco diversos.

4. Reducción de la complejidad computacional
Con menos datos, el modelo puede entrenarse más rápido y consumir menos recursos computacionales, lo cual es ventajoso en entornos con restricciones de hardware.

5. Posible pérdida de información clave
Dependiendo de cómo se definan los valores atípicos, la eliminación podría llevar a la pérdida de información valiosa que es importante para ciertas categorías de datos, afectando la capacidad del modelo para detectar escenarios inusuales pero relevantes.








## Gráficos luego de eliminar datos atípicos

En las siguientes gráficas, puedes observar las diferencias con respecto a las del apartado inicial "Analizar Patrones Anómalos".

*Graficar la región(Regiao) en función de la edad(Idade), del conjunto de datos `df_insurance`, utilizando la función `plot_boxplot_violinplot`.*
"""

#Write your code here
print("Número de registros después de eliminar valores atípicos:", df_insurance.shape[0])

""" *Graficar la región(Regiao) en función de los ingresos(Renda), del conjunto de datos `df_insurance`, utilizando la función `plot_boxplot_violinplot`.*"""

#Write your code here, add you plot using ´plot_boxplot_violinplot´
plot_boxplot_violinplot('Regiao', 'Idade', df_insurance)


"""## Pregunta
*¿Cómo crees que la eliminación de datos atípicos ha afectado la distribución y los patrones observados en las gráficas? ¿Qué cambios específicos puedes identificar en los datos después de esta eliminación?*

1. Reducción de la dispersión de los datos
Antes de eliminar los valores atípicos, los datos pueden haber mostrado una dispersión amplia, con puntos alejados de la tendencia central. Después de la eliminación, es probable que la dispersión se reduzca, mostrando una mayor concentración de los datos alrededor de la media.

2. Mayor homogeneidad en la distribución
Los gráficos como histogramas o diagramas de caja podrían mostrar una distribución más uniforme y simétrica después de la eliminación de atípicos, eliminando sesgos que estos valores extremos podrían haber introducido.

3. Cambios en los valores estadísticos clave
La media y la mediana pueden haberse ajustado, reflejando mejor la tendencia general de los datos. Además, la desviación estándar podría haberse reducido, indicando una menor variabilidad.

4. Alteración en la forma de las distribuciones
Es posible que la eliminación de valores atípicos haya cambiado la forma de las distribuciones de algunas variables, volviéndolas más normales (campana de Gauss) si los atípicos estaban causando colas largas en la distribución.

5. Mejora en la interpretabilidad de los datos
Las relaciones entre las variables pueden hacerse más claras y los patrones más fáciles de identificar, lo que facilita la interpretación y el análisis de correlaciones entre variables.

6. Pérdida de algunos detalles importantes
En algunos casos, la eliminación de valores atípicos podría haber eliminado información crítica sobre subgrupos o patrones únicos que podrían haber sido relevantes para el análisis o la toma de decisiones.



Cambios específicos identificables en los datos:
En los diagramas de caja (boxplots): Se reducen los valores fuera del rango intercuartílico (IQR), mostrando un rango más acotado de valores.
En los gráficos de dispersión: Se observa una menor cantidad de puntos extremos, con una relación más clara entre las variables.
En los histogramas: La distribución puede volverse más centrada y simétrica sin colas largas.






## Normalización y Escalado
### Estandarización:
Vamos a convertir los datos numéricos a una escala común. Para esto, vamos a aplicar la estandarización sobre las columnas numéricas "Idade" y "Renda". Luego, para ajustar la escala, vamos a utilizar las clases StandardScaler y ColumnTransformer. Debes implementar el código necesario para realizar la conversión.

Para la clase DataScaleImputer, debes investigar un poco cómo realizar la conversión a una escala estándar (StandardScaler) mediante el uso de la clase ColumnTransformer. Puedes revisar la documentación oficial de sklearn.

Otra opción es crear una función que permita estandarizar las características eliminando la media y escalando a varianza unitaria. Esto se calcula como: z = (x - u) / s
"""

# Custom class to impute and scale data
class DataScaleImputer(BaseEstimator, TransformerMixin):
    def __init__(self, columns):
        self.columns = columns  # Columns to be scaled
        self.scaler = StandardScaler()  # Initialize the StandardScaler

    def fit(self, X, y=None):
        # Fit the scaler on the specified columns
        self.scaler.fit(X[self.columns])
        return self  # Return the transformer

    def transform(self, X):
        data = X.copy()  # Make a copy of the input DataFrame to avoid modifying the original

        # Create a ColumnTransformer that will apply StandardScaler only to the specified columns
        # Write you code here, change None by custom transformer
        plot_boxplot_violinplot('Regiao', 'Renda', df_insurance)
        transformer = ColumnTransformer(
    transformers=[('scaler', StandardScaler(), self.columns)],
    remainder='passthrough'
)

        # Apply the transformer to the data
        X_transform = transformer.fit_transform(data)

        # Convert the result to a DataFrame to maintain the column labels
        X_imputed_df = pd.DataFrame(data=X_transform, columns=self.columns + [col for col in X.columns if col not in self.columns])


        # Replace the original columns in 'data' with the scaled columns
        data[self.columns] = X_imputed_df[self.columns]

        return data.dropna()  # Return the transformed DataFrame

"""*Ejecuta la transformación utilizando la clase `DataScaleImputer` y asigna el resultado a `df_insurance`*"""

# Write you code here
scaler = DataScaleImputer(columns=['Idade', 'Renda'])

"""*Imprime las estadísticas básicas del conjunto de datos df_insurance, ubásicas utilizando el método `describe()`*"""

# Write you code here
df_insurance = scaler.fit_transform(df_insurance)
print(df_insurance.describe())

"""## Pregunta
*¿Cuáles otras técnicas conoces que pueden ser utilizadas para escalar o normalizar los datos? Menciona dos.*

## Unificación de conjuntos de datos

Vamos a unificar diferentes conjuntos de datos (`df_insurance`, `df_retailbank` y `df_investment`) para crear un nuevo DataFrame. Utilizaremos la función `merge` de Pandas, identificando previamente el atributo que nos permitirá integrar estos conjuntos como uno solo. El resultado final se asignará a la variable `data_frame_merged`. A continuación, mostraremos los primeros 10 registros.

*Utiliza la función `merge` de Pandas para fusionar los conjuntos de datos en uno solo, asignándolo a la variable `data_frame_merged`.*
"""

# Write you code here
data_frame_merged = pd.merge(pd.merge(df_retailbank, df_investment, on='ID'), df_insurance, on='ID')


"""*Imprime la cantidad total de registros después de realizar el merge entre los conjuntos de datos.*"""

# Write you code here
print("Número total de registros después de la unificación:", data_frame_merged.shape[0])


"""*Observamos una visión estadística rápida de los datos mediante la función `describe`.*"""

# Write you code here
print(data_frame_merged.describe())

"""*Verifica si hay datos faltantes en el DataFrame resultante.*"""

# Write you code here
print("Valores faltantes después de la unificación:")
print(get_nan_values(data_frame_merged))
















"""# Correcion nombres columnas

Como has notado, se presentan ciertos inconvenientes en los nombres de las columnas. A continuación, intentaremos resolver estos errores identificando y corrigiendo espacios adicionales u otros problemas.
"""

data_frame_merged = data_frame_merged.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))
data_frame_merged.info()

"""## Finalización tramiento de datos

### Tratamiento de datos para modelos de Machine Learning

Como último paso, es necesario ejecutar el siguiente tratamiento a los datos con el objetivo de prepararlos para nuestros modelos de Machine Learning. Seguiremos los siguientes pasos:

1. **Creación de la variable a predecir:** Se creará una nueva columna llamada "tipo_financiamiento", que será la variable a predecir por nuestros modelos de Machine Learning. Esta columna representará el tipo de financiamiento, permitiendo identificar si corresponde a una casa, un carro, ambos o ninguno. El siguiente proceso se ejecutará sobre el `data_frame_merged`, generando un nuevo DataFrame llamado `data_frame_tipo_financiamiento` con la columna adicional "tipo_financiamiento".

2. **Etiquetado de columnas categóricas:** Se etiquetarán las columnas categóricas como multi label. Las columnas identificadas son "AGE_RANGE", "INCOME_RANGE", "tipo_financiamiento" y "Regiao".

3. **Eliminación de la columna "ID":** Se eliminará la columna utilizada como identificador.

4. **Conversión de valores binarios:** Se convertirán todas las columnas con valores 'F' o 'T' a tipos de datos numéricos 0 y 1, respectivamente.
"""

class OneHotDecoderImputer(BaseEstimator, TransformerMixin):
    def __init__(self, columns, label_column_name):
        """
        Initialize the OneHotDecoderImputer.

        Parameters:
        - columns: list of str, names of columns to be converted from one-hot encoding
        - label_column_name: str, name of the new label column
        """
        self.columns = columns  # List of column names to be converted from one-hot encoding
        self.label_column_name = label_column_name
        self.label_encoders = {}  # Dictionary to store label encoders for each column

    def fit(self, X, y=None):
        """
        Fit the label encoders on the specified columns.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - self: OneHotDecoderImputer, the transformer instance
        """
        for col in self.columns:
            encoder = LabelEncoder()
            encoder.fit(X[col])
            self.label_encoders[col] = encoder
        return self  # Return the transformer instance

    def get_financing_type_name_from_row(self, row):
        """
        Get the financing type name from a one-hot encoded row.

        Parameters:
        - row: pd.Series, a row of one-hot encoded data

        Returns:
        - str or None, the name of the financing type or None if not found
        """
        total_financing_types = row.sum()
        if total_financing_types == len(row):
            return "Ambos"
        if total_financing_types == 0:
            return "Ninguno"
        for col_name, value in row.items():
            if value == 1:
                return col_name
        return None

    def transform(self, X):
        """
        Convert one-hot encoded columns to a single label column.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - X_transformed: pd.DataFrame, the DataFrame with the new label column
        """
        # Create a copy of the input DataFrame to keep the original data
        X_transformed = X.copy()

        # Encode the specified columns using the fitted label encoders
        for col in self.columns:
            X_transformed[col] = self.label_encoders[col].transform(X[col])

        # Create the label column by applying the method to each row
        X_transformed[self.label_column_name] = X_transformed[self.columns].apply(lambda row: self.get_financing_type_name_from_row(row), axis=1)

        return X_transformed.drop(columns=self.columns)
class BooleanToNumeric(BaseEstimator, TransformerMixin):
    def __init__(self):
        """
        Initialize the BooleanToNumeric transformer.
        """
        pass

    def fit(self, X, y=None):
        """
        Fit the BooleanToNumeric transformer.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - self: BooleanToNumeric, the transformer instance
        """
        return self

    def transform(self, X):
        X_transformed = X.replace({"T": 1, "F": 0})
        pd.set_option('future.no_silent_downcasting', True)
        return X_transformed  # Indentado correctamente

class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, columns=None):
        """
        Initialize the MultiColumnLabelEncoder.

        Parameters:
        - columns: array of str, names of columns to encode. If None, encode all columns.
        """
        self.columns = columns
        self.label_encoders = {}

    def fit(self, X, y=None):
        """
        Fit the label encoders on the specified columns.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - self: MultiColumnLabelEncoder, the transformer instance
        """
        if self.columns is None:
            self.columns = X.columns
        for col in self.columns:
            self.label_encoders[col] = LabelEncoder().fit(X[col])
        return self

    def transform(self, X):
        """
        Transform the specified columns using the fitted label encoders.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - X_transformed: pd.DataFrame, the DataFrame with transformed columns
        """
        X_transformed = X.copy()
        for col in self.columns:
            X_transformed[col] = self.label_encoders[col].transform(X[col])
        return X_transformed

    def fit_transform(self, X, y=None):
        """
        Fit label encoders on the specified columns and transform the DataFrame.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - X_transformed: pd.DataFrame, the DataFrame with transformed columns
        """
        return self.fit(X, y).transform(X)

    def inverse_transform(self, X):
        """
        Reverse the encoding back to the original values.

        Parameters:
        - X: pd.DataFrame, the DataFrame with encoded columns

        Returns:
        - X_inverse: pd.DataFrame, the DataFrame with original values
        """
        X_inverse = X.copy()
        for col in self.columns:
            X_inverse[col] = self.label_encoders[col].inverse_transform(X[col])
        return X_inverse

class DropColumns(BaseEstimator, TransformerMixin):
    def __init__(self, columns=None):
        """
        Initialize the DropColumns transformer.

        Parameters:
        - columns: list of str, names of columns to drop from the DataFrame
        """
        self.columns = columns

    def fit(self, X, y=None):
        """
        Fit the DropColumns transformer.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - self: DropColumns, the transformer instance
        """
        return self

    def transform(self, X):
        """
        Transform the input DataFrame by dropping specified columns.

        Parameters:
        - X: pd.DataFrame, the input DataFrame

        Returns:
        - X_transformed: pd.DataFrame, the transformed DataFrame
        """
        X_transformed = X.drop(columns=self.columns, errors='ignore')
        return X_transformed

"""Definimos el pipeline para ajustar los datos al formato requerido para resolver el ejercicio."""

pipeline_data_preparation = Pipeline([
    ("one_hote_to_label" ,OneHotDecoderImputer(columns=["FinanciamentoCasa","FinanciamentoCarro"], label_column_name = "tipo_financiamiento" )),
    ("label_encode", MultiColumnLabelEncoder(columns=["AGE_RANGE","INCOME_RANGE","tipo_financiamiento","Regiao"])),
    ('drop_columns', DropColumns(columns=["ID"])),
    ('boolean_numeric', BooleanToNumeric())
])

"""*Ejecuta el pipeline para ajustar los datos y asignarlos a la variable `data_frame_tipo_financiamiento`.*"""

data_frame_tipo_financiamiento = pipeline_data_preparation.fit_transform(data_frame_merged)
data_frame_tipo_financiamiento.head(10)

"""Obtenemos las etiquetas por tipo de financiamiento y asignamos a la varabile `le_tipo_financiamiento_mapping`."""

le = pipeline_data_preparation[1].label_encoders["tipo_financiamiento"]
le_tipo_financiamiento_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
tipo_financiamiento_mapping = {v: k for k, v in le_tipo_financiamiento_mapping.items()}
tipo_financiamiento_mapping

"""*Imprime las estadísticas básicas del conjunto de datos `data_frame_tipo_financiamiento`*"""


# Write you code here
print(data_frame_tipo_financiamiento.head(10))

# Write you code here
print(data_frame_tipo_financiamiento.describe())









"""## Cierre tratamiento de datos
Es crucial comprender que el tratamiento de datos no es solo una etapa preliminar, sino un proceso continuo que puede influir significativamente en el rendimiento y la precisión de los modelos de Machine Learning. Al abordar de manera efectiva problemas como valores faltantes, valores atípicos y errores de formato, estamos creando un conjunto de datos robusto y confiable, lo que a su vez potencia la capacidad predictiva de nuestros modelos.

Hasta este punto, hemos completado varios pasos relacionados con el tratamiento y la limpieza de datos. Ahora vamos a continuar con el desarrollo de los diferentes algoritmos de Machine Learning.

*Exporta el DataFrame data_frame_label a un archivo CSV sin incluir el índice*
"""

# Write you code here
data_frame_tipo_financiamiento.to_csv('predicciones.csv', index=False)

"""# **Pregunta 3 - Creación de modelos de Machine Learning**

# Selección de modelo:
Identifica el tipo de problema de aprendizaje (clasificación, regresión, agrupamiento, etc.) y selecciona los modelos más adecuados para tu problema.
Experimenta con diferentes algoritmos de machine learning y ajusta sus hiperparámetros para encontrar la mejor combinación.
1. Entrenamiento de modelos:
Entrena por lo menos 3 modelos utilizando los datos de entrenamiento. Ajusta los parámetros del modelo utilizando algoritmos de optimización como la descenso del gradiente, búsqueda de cuadrícula, o búsqueda aleatoria.
Valida el modelo utilizando los datos de validación y ajusta los parámetros según sea necesario para evitar el sobreajuste (overfitting).
2. Evaluación del modelo:
Evalúa el rendimiento del modelo utilizando métricas apropiadas para tu problema (precisión, recall, F1-score, matriz de confusión, etc.).
Utiliza técnicas de validación cruzada para obtener estimaciones más robustas del rendimiento del modelo.

## Pregunta
*¿Cuál es el tipo de problema que estás enfrentando: clasificación o regresión? Imprime o grafica el conteo de valores que corresponde a la columna `data_frame_tipo_financiamiento`.*

El tipo de problema que estamos enfrentando es un problema de clasificación, ya que la columna data_frame_tipo_financiamiento representa categorías discretas de financiamiento, como casa, carro, ambos o ninguno. En un problema de clasificación, el objetivo es asignar instancias a una de varias categorías predefinidas.
Dado que el objetivo es clasificar diferentes tipos de financiamiento, este es un problema de clasificación supervisada, donde se entrenará un modelo para predecir la categoría correcta basándose en las características disponibles.

"""

# Write you code here
print("Tipo de problema: Clasificación")

# Write you code here, add your custom plot
import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='tipo_financiamiento', data=data_frame_tipo_financiamiento)
plt.title('Distribución de tipos de financiamiento')
plt.xlabel('Tipo de financiamiento')
plt.ylabel('Cantidad')
plt.show()




















"""## Pasos para el entrenamiento de modelos

A continuación, desarrolla los siguientes pasos para cada uno de los modelos sobre el conjunto de datos `data_frame_tipo_financiamiento`:
* **División del conjunto de datos:** Divide los datos en conjuntos de entrenamiento y prueba. El conjunto de entrenamiento se utiliza para entrenar el modelo, mientras que el conjunto de prueba se utiliza para evaluar su rendimiento.

* **Selección de modelo:** Elige el algoritmo de aprendizaje automático más adecuado para tu problema. Esto depende del tipo de problema (regresión, clasificación, clustering, etc.), el tamaño y la naturaleza de los datos, y los requisitos de rendimiento.

* **Entrenamiento del modelo:** Utiliza el conjunto de entrenamiento para ajustar los parámetros del modelo. Durante este proceso, el modelo aprenderá a mapear las características de entrada a las etiquetas de salida.

* **Validación del modelo:** Evalúa el rendimiento del modelo utilizando el conjunto de prueba. Esto te permite verificar si el modelo generaliza bien a datos no vistos y detectar posibles problemas de sobreajuste o subajuste.

### **Pasos para el entrenamiento del modelo - (Nombre Modelo)**
"""

# Load your dataset
# Assuming your data is stored in a DataFrame called 'data_frame_tipo_financiamiento'
# and the target variable is in a column called 'tipo_financiamiento'
# Replace 'data_frame_tipo_financiamiento' and 'tipo_financiamiento' with your actual DataFrame and column names
# Write you code here

X = data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento'])  # Features
y = data_frame_tipo_financiamiento['tipo_financiamiento']  # Target variable

# Split the data into training and testing sets using startified_train_test_split
# You can adjust the test_size parameter as needed
# 'random_state' ensures reproducibility of results
# Write you code here

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Create the custom model
# You can customize the parameters based on your requirements
# Write you code here
model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

# Train the model on the training data
# Write you code here
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)
# Write you code here
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)













"""### **Evaluación del modelo - (Nombre Modelo)**"""

# Evaluate accuracy the model
# Write you code here
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Plot accuracy the model over the time - use plot_accuracy_scores
#plot_accuracy_scores(rf_model,X_train,y_train,X_test,y_test,nparts=5,jobs=2)
plot_accuracy_scores(model, X_train, y_train, X_test, y_test, nparts=5, jobs=2)

# Print classifitacion report using classification_report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plot confusion matrix using plot_confusion_matrix
cm = confusion_matrix(y_test, y_pred)
plot_confusion_matrix(cm, tipo_financiamiento_mapping)






"""### **Pasos para el entrenamiento del modelo  a comparar - (LogisticRegression)**"""

# Load your dataset
# Assuming your data is stored in a DataFrame called 'data_frame_tipo_financiamiento'
# and the target variable is in a column called 'tipo_financiamiento'
# Replace 'data_frame_tipo_financiamiento' and 'tipo_financiamiento' with your actual DataFrame and column names
X = data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento'])  # Features
y = data_frame_tipo_financiamiento['tipo_financiamiento']  # Target variable

# Split the data into training and testing sets
# You can adjust the test_size parameter as needed
# 'random_state' ensures reproducibility of results
X_train, X_test, y_train, y_test = startified_train_test_split(X, y, test_size=0.2, random_state=42)

# Create the Random LogisticRegression
# You can customize the parameters based on your requirements
lr_model = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='multinomial', n_jobs=None, penalty='l2',
                   random_state=1355, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)

# Train the model on the training data
lr_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = lr_model.predict(X_test)










"""### **Evaluación del modelo - (LogisticRegression)**"""

# Evaluate accuracy the model
# Entrenar el modelo
model.fit(X_train, y_train)

# Realizar predicciones
y_pred = model.predict(X_test)

# Evaluar la precisión del modelo
accuracy = accuracy_score(y_test, y_pred)
print("Model accuracy:", accuracy)
# Plot accuracy the model over the time
plot_accuracy_scores(lr_model,X_train,y_train,X_test,y_test,nparts=5,jobs=2)

# Print classifitacion report
clas_report=classification_report(y_test,y_pred,labels=np.unique(y_pred), digits=6)
print(clas_report)

# Plot confusion matrix
plot_confusion_matrix(confusion_matrix(y_test, y_pred),tipo_financiamiento_mapping)








"""## Preguntas
* *¿Puedes comparar los modelos y determinar cuál de ellos tiene un mejor rendimiento en términos de exactitud?*

* *¿Logran los modelos etiquetar todas las clases de forma precisa? ¿Qué estrategias podrían aplicarse para mejorar este aspecto?*

## Extracción de características y Análisis de Componentes Principales(PCA)

Aquí está la corrección:

Ahora vamos a desarrollar validaciones para ver cuáles características son más relevantes para el modelo. Para esto, debes a implementar una función llamada  `plot_correlations` que te permita graficar las correlaciones del DataFrame `data_frame_tipo_financiamiento`.
"""

def plot_correlations(df_temp):
    """
    Function to plot the correlation heatmap of a DataFrame.
    
    Parameters:
    df_temp (pd.DataFrame): DataFrame containing the data to analyze.
    """
    plt.figure(figsize=(12, 8))
    correlation_matrix = df_temp.corr()
    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)
    plt.title('Correlation Heatmap')
    plt.show()
    pass

#Write your code here, plot using plot_correlations
plot_correlations(data_frame_tipo_financiamiento)









"""## Pregunta
* *¿Puedes identificar cuáles columnas son más relevantes y por qué?*

La siguiente función, `get_most_important_features`, nos permite extraer aquellas n columnas más relevantes a partir de la matriz de correlación.
"""

def get_most_important_features(correlation_matrix, target_column, n=5):
    """
    Get the top N most important features based on their absolute correlation values.

    Parameters:
    - correlation_matrix: pd.DataFrame, the correlation matrix
    - target_column: str, the name of the target variable column
    - n: int, the number of top features to return

    Returns:
    - top_features: list, the top N most important feature names
    """
    # Get the absolute correlation values with the target variable
    correlation_with_target = correlation_matrix[target_column].abs().sort_values(ascending=False)

    # Exclude the target variable itself
    correlation_with_target = correlation_with_target.drop(target_column)

    # Get the top N most important features
    top_features = correlation_with_target.head(n).index.tolist()

    return top_features

"""*Utiliza la función `get_most_important_features` e imprime las primeras 6 columnas más relevantes del conjunto de datos que están relacionadas con la variable a predecir.*

"""

#Write your code here
correlation_matrix = data_frame_tipo_financiamiento.corr()
top_features = get_most_important_features(correlation_matrix, 'tipo_financiamiento', n=6)
print("Top 6 most important features:", top_features)








"""# Análisis de Componentes Principales(PCA)

## Pregunta
*¿Qué es el análisis de componentes principales y cuál es su utilidad al implementar modelos de machine learning?*


El Análisis de Componentes Principales (PCA) es una técnica de reducción de dimensionalidad utilizada en el análisis de datos y machine learning. Su objetivo principal es transformar un conjunto de variables posiblemente correlacionadas en un nuevo conjunto de variables llamadas componentes principales, las cuales son linealmente independientes entre sí.

PCA funciona buscando direcciones de máxima variabilidad en los datos y proyectándolos sobre un espacio de menor dimensión, manteniendo la mayor cantidad de información posible.

El proceso clave en PCA implica los siguientes pasos:

    Estandarización: Los datos son normalizados para que todas las variables tengan la misma escala (media = 0, varianza = 1).
    
    Cálculo de la matriz de covarianza: Se mide la relación entre las variables.

    Obtención de los eigenvalores y eigenvectores: Se identifican las direcciones principales (componentes) de los datos.
    
    Proyección de los datos: Se transforman los datos originales a las nuevas dimensiones principales.


Utilidad del PCA en modelos de Machine Learning
El PCA tiene múltiples aplicaciones y beneficios en la implementación de modelos de machine learning:

    Reducción de dimensionalidad:
        Permite reducir el número de características manteniendo la mayor cantidad de información posible.
        Facilita el manejo de datos de alta dimensionalidad, mejorando la eficiencia computacional.

    Eliminación de colinealidad:
        Transforma variables correlacionadas en nuevas variables no correlacionadas, lo que puede mejorar el rendimiento de modelos lineales.

    Mejora del rendimiento del modelo:
        Al reducir el número de características irrelevantes o redundantes, se reduce el riesgo de sobreajuste, permitiendo que los modelos generalicen mejor en datos nuevos.
        
    Visualización de datos:
        Al proyectar datos en 2 o 3 dimensiones, facilita la exploración y visualización de conjuntos de datos complejos.

    Preprocesamiento para clustering y clasificación:
        PCA puede ser útil como paso previo a algoritmos de agrupamiento o clasificación, proporcionando una representación más significativa de los datos.



*Modifica la función `create_pca_model`. Los parámetros de entrada son el conjunto de datos sin la variable a predecir. Se creará un modelo de Análisis de Componentes Principales (PCA), el cual tendrá como parámetro el número N de componentes a identificar. El resultado será el modelo exportado y la transformación hacia las componentes principales luego de evaluar el modelo.*

* Esta función toma como entrada `X_train`, que representa las características del conjunto de entrenamiento sin la variable objetivo, y `n_components`, que indica el número de componentes principales que se desea conservar.

* Dentro de la función, se instancia un objeto PCA con el número de componentes especificado. Luego, se ajusta este objeto PCA a las características del conjunto de entrenamiento para determinar las características transformadas.

* Estas características transformadas se almacenan en un DataFrame llamado `X_principal`, que luego se devuelve junto con el objeto PCA ajustado (`pca_model`) como salida de la función.
"""

def create_pca_model(X_train, n_components):
    """
    Create a Principal Component Analysis (PCA) model.

    Parameters:
    X_train (DataFrame): The training dataset without the target variable.
    n_components (int): The number of principal components to identify.

    Returns:
    pca_model (PCA): The fitted PCA model.
    X_principal (DataFrame): The transformed features into principal components.
    """
    # Instantiate PCA
    #Write your code here
    
    pca_model = PCA(n_components=n_components)

    # Fit PCA to the training data and transform features
    #Write your code here
    X_principal = None
    X_principal = pca_model.fit_transform(X_train)

    # Return pca_model,X_principal
    return pca_model, pd.DataFrame(X_principal)








"""Llamamos a la función `create_pca_model`, pasando como argumentos el DataFrame `data_frame_tipo_financiamiento` y `n_components` igual a 10, para determinar las 10 componentes principales del conjunto de datos. Luego, graficamos la varianza acumulada."""

pca_model,X_principal = create_pca_model(data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento']),n_components=10)
plot_pca_cumulative_variance(pca_model)

"""A continuación, obtenemos la lista de los N componentes principales."""

df_pca_components = get_pca_components(pca_model,data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento']).columns)
df_pca_components

"""## Pregunta
*Compara las variables obtenidas después de realizar el PCA en el conjunto de datos con las variables identificadas a través de la matriz de confusión. ¿Has encontrado coincidencias entre las variables y qué conclusiones puedes extraer de esto?*








Vamos a graficar la curva conocida como codo (elbow curve) utilizando la función `plot_elbow_curve_pca`.
"""

plot_elbow_curve_pca(X_principal)

"""## Pregunta
*Primero, investiga para qué sirve la curva conocida como codo (elbow curve). Luego, responde a la pregunta: ¿Cuántos componentes principales (columnas) puedes sugerir que sean utilizados por algún modelo de Machine Learning?*

*Establece el valor para la variable `n_components_pca`, luego ejecuta el modelo de aprendizaje, que incluye una tarea de reducción de la dimensionalidad mediante PCA (Análisis de Componentes Principales).*
"""

n_componenets_pca = 2

X = data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento'])
y = data_frame_tipo_financiamiento['tipo_financiamiento']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the steps of the pipeline
steps = [

    ('pca', PCA(n_components=n_componenets_pca)),   # Apply PCA to reduce dimensionality to 2 components
    ('clf', GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=447, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False))  # Example classifier
]

# Create the pipeline
pipeline = Pipeline(steps)

# Train the model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)

# Print classifitacion report
clas_report=classification_report(y_test,y_pred,labels=np.unique(y_pred), digits=6)
print(clas_report)

# Plot confusion matrix
plot_confusion_matrix(confusion_matrix(y_test, y_pred),tipo_financiamiento_mapping)

"""Interaction Terms:

Create new features that capture the interactions between existing features. For instance, combining pairs of binary features using logical operations like AND, OR might uncover useful patterns.

## Implementación del tratamiento de datos desbalanceados
"""

X = data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento'])
y = data_frame_tipo_financiamiento['tipo_financiamiento']

"""A continuación, se muestra una gráfica que ilustra la presencia de datos desbalanceados.



"""

conteo_tipo_financiamiento_label = y.value_counts().rename(index=tipo_financiamiento_mapping)
conteo_tipo_financiamiento_label.plot.pie()
y.value_counts()

"""Para abordar el problema, vamos a comenzar reduciendo la variable que tiene mayor presencia y luego crearemos nuevos datos sintéticos para que los datos con menor presencia tengan la misma representatividad."""

# Define the steps of the pipeline
# Calculating class counts
class_counts = data_frame_tipo_financiamiento['tipo_financiamiento'].value_counts()

# Setting sampling strategy
sampling_strategy = {3:int(class_counts.max() * 0.30), 1:int(class_counts[1]*0.20)}

# Undersampling with RandomUnderSampler
rand_under = RandomUnderSampler(sampling_strategy=sampling_strategy)

# Oversampling with SMOTE
smote_over = SMOTE(sampling_strategy='not majority', k_neighbors=5, random_state=42)

# Steps for addressing imbalance
steps_imbalance = [
    ('sampling_under', rand_under),
    ('sampling', smote_over)
]

# Create the pipeline
pipeline_fix_imbalance = Pipeline(steps_imbalance)
pipeline_fix_imbalance.fit(X, y)

# Resample the data
X_reshaped, y_reshaped = pipeline_fix_imbalance.fit_resample(X, y)

"""*Implementa un gráfico tipo pie que muestre cómo lucen los datos después de realizar el tratamiento para abordar el desbalance.*"""

#Write your code here
#conteo_tipo_financiamiento_label = y_reshaped.value_counts().rename(index=tipo_financiamiento_mapping)
#conteo_tipo_financiamiento_label.plot.pie()
#y_reshaped.value_counts()

conteo_tipo_financiamiento_label = y_reshaped.value_counts().rename(index=tipo_financiamiento_mapping)
conteo_tipo_financiamiento_label.plot.pie()
plt.show()
print(y_reshaped.value_counts())






"""*Separa los datos en conjuntos de entrenamiento y test utilizando la función `startified_train_test_split()`. Luego, implementa un modelo que haga uso del siguiente clasificador. Puedes probar modificando los hiperparámetros y evaluar los resultados. También puedes optar por modificar los parámetros de las clases `RandomUnderSampler` y `SMOTE` del paso anterior.*


```
GradientBoostingClassifier(
        ccp_alpha=0.0,
        criterion='friedman_mse',
        init=None,
        learning_rate=0.1,
        loss='log_loss',
        max_depth=3,
        max_features=None,
        max_leaf_nodes=None,
        min_impurity_decrease=0.0,
        min_samples_leaf=1,
        min_samples_split=2,
        min_weight_fraction_leaf=0.0,
        n_estimators=100,
        n_iter_no_change=None,
        random_state=8860,
        subsample=1.0,
        tol=0.0001,
        validation_fraction=0.1,
        verbose=0,
        warm_start=False)
```


"""

# Split the data into training and testing sets with stratification
# Stratification ensures that the class distribution is preserved in both training and testing sets
# Write your code here


#Write your code here
X_train, X_test, y_train, y_test = startified_train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)

#Write your code here
clf = GradientBoostingClassifier(
    ccp_alpha=0.0,
    criterion='friedman_mse',
    init=None,
    learning_rate=0.1,
    loss='log_loss',
    max_depth=3,
    max_features=None,
    max_leaf_nodes=None,
    min_impurity_decrease=0.0,
    min_samples_leaf=1,
    min_samples_split=2,
    min_weight_fraction_leaf=0.0,
    n_estimators=100,
    n_iter_no_change=None,
    random_state=8860,
    subsample=1.0,
    tol=0.0001,
    validation_fraction=0.1,
    verbose=0,
    warm_start=False
)

# Entrenar el modelo
clf.fit(X_train, y_train)

# Realizar predicciones
y_pred = clf.predict(X_test)

# Evaluar la precisión
accuracy = accuracy_score(y_test, y_pred)
print("Model Accuracy:", accuracy)


# Print classification report
print(classification_report(y_test, y_pred))

# Plot confusion matrix
plot_confusion_matrix(confusion_matrix(y_test, y_pred), tipo_financiamiento_mapping)








# Define the steps for the pipeline
steps_gradient_boost = [
    ('sampling_under', rand_under),  # Undersampling
    ('sampling_over', smote_over),    # Oversampling
    ('clf', GradientBoostingClassifier(
        ccp_alpha=0.0,
        criterion='friedman_mse',
        init=None,
        learning_rate=0.1,
        loss='log_loss',
        max_depth=3,
        max_features=None,
        max_leaf_nodes=None,
        min_impurity_decrease=0.0,
        min_samples_leaf=1,
        min_samples_split=2,
        min_weight_fraction_leaf=0.0,
        n_estimators=100,
        n_iter_no_change=None,
        random_state=8860,
        subsample=1.0,
        tol=0.0001,
        validation_fraction=0.1,
        verbose=0,
        warm_start=False))  # Example classifier
]

# Create the pipeline for Gradient Boosting
# Write your code here
pipeline_gradient_boost = Pipeline(steps_gradient_boost)

# Train the model using fit
# Write your code here
pipeline_gradient_boost.fit(X_train, y_train)

# Make predictions
# Write your code here
y_pred = pipeline_gradient_boost.predict(X_test)










"""Evaluemos los resultados del modelo."""

# Print classifitacion report
clas_report=classification_report(y_test,y_pred,labels=np.unique(y_pred), digits=6)
print(clas_report)

# Plot confusion matrix
plot_confusion_matrix(confusion_matrix(y_test, y_pred),tipo_financiamiento_mapping)




"""# Pregunta 4
* *¿Cuál de los modelos consideras que es más eficiente en términos de rendimiento y por qué?*
* *Luego de evaluar los diferentes modelos, como científico de datos, ¿cuál sugerirías implementar y por qué? Justifica tu respuesta.*


* *Investiga qué otras opciones pueden ser utilizadas para enfrentar el problema de datos desbalanceados e implementa un ejemplo.*

Existen varias estrategias para abordar el problema de datos desbalanceados:

Técnicas de remuestreo:

Submuestreo (undersampling): Reducir la cantidad de registros en la clase mayoritaria.
Sobremuestreo (oversampling): Aumentar la cantidad de registros en la clase minoritaria (por ejemplo, con SMOTE - Synthetic Minority Over-sampling Technique).
Modificación de la función de pérdida:

Ajustar los pesos de las clases para penalizar más los errores en la clase minoritaria.
Algoritmos especializados:

Uso de algoritmos como BalancedRandomForestClassifier o WeightedSVM.

Ejemplo de implementación con SMOTE:

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from collections import Counter

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Aplicar SMOTE para sobremuestreo
smote = SMOTE(sampling_strategy='minority', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Contar la distribución de clases después del balanceo
print("Distribución de clases después del SMOTE:", Counter(y_resampled))





* *Investiga qué son los modelos de ensamble e implementa un corto ejemplo.*

Los modelos de ensamble combinan varios modelos base para mejorar la precisión y la estabilidad de las predicciones. Se dividen en:

Bagging (Bootstrap Aggregating): Entrena múltiples instancias de un mismo modelo con subconjuntos de los datos y promedia los resultados (ej. Random Forest).
Boosting: Entrena modelos secuenciales en los errores del modelo anterior (ej. Gradient Boosting, AdaBoost).
Stacking: Combina varios modelos base y entrena un modelo meta para optimizar las predicciones finales.

Ejemplo de implementación de un modelo de ensamble con Random Forest:

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Entrenamiento del modelo de ensamble (Random Forest)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predicciones y evaluación
y_pred_rf = rf_model.predict(X_test)
print("Accuracy del modelo de ensamble:", accuracy_score(y_test, y_pred_rf))


Los modelos de ensamble suelen ser más robustos y generalizan mejor que los modelos individuales, especialmente en conjuntos de datos con alta variabilidad.

"""